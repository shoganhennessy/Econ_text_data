---
title: "Working Papers in Economics: the NBER Series"
author: "Senan Hogan-Hennesssy"
date: "`r format(Sys.time(), '%d %b %Y')`"
output:
  html_document: default
  pdf_document: default
---
<style>
body {
text-align: justify}
</style>

## UNFINISED, TO BE COMPLETED OVER THE NEXT WEEK
<!-- Look at blog post   : http://varianceexplained.org/r/trump-tweets/#fnref:fullcode --> 
<!-- Look at source code : https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2016-08-09-trump-tweets.Rmd --> 

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = F)
library(tidyverse)
library(data.table)
library(stringi)
library(ggplot2)
theme_set(theme_bw())

# Load the raw working papers data
Papers.data <- fread('../Data/NBER_data/Papers_data/Working_papers_data_new.csv')
Papers.data <- Papers.data %>% filter(title != 'Paper Not Found', !is.na(date), date != '') # FIlter empty entries 
```

Publishing in a main goal of academic economists, but rarely is it as simple
as write the paper then publish.
Projects can take years and often transform over the work, data analysis and 
dissemination of results.
An important part of gauging the impact of research in progress is releasing a working paper.
The National Bureau of Economics Research (NBER) hosts a heavily circulated 
[series](https://www.nber.org/papers)
for its affiliates to register working papers, and the series includes pre-published versions
of some very famous research papers and some heavily cited working papers themselves.

> ``We owe a tremendous amount to the NBER working Paper series.
> It has filled a huge gap in in our ability to dissemniate knowledge before publication [...], 
> sharing knowledge as it is produced.''
> 
> -- Claudia Goldin, 2014 at a conference on the NBER history.
<!-- Presentation on the series : https://www.nber.org/WPat20K/summary.html# --> 


### The dataset

I have retreieved the listing of all NBER working papers, a collection of 
`r nrow(Papers.data)` papers  (as of `r as.Date(max(Papers.data$date)) %>% format("%d %b %Y")`),
starting with a small number of technical reports in 1973
to hundreds of modern research papers in the 2000s.

```{r, echo=F, fig.height = 4, fig.width = 7, fig.align = 'center'}
# SHow count per year
Papers.data %>%
  mutate(date = as.Date(date),
         year = as.integer(format(date, format = '%Y'))) %>%
  group_by(year) %>%
  summarise(count = n()) %>%
  ggplot(aes(x = year, y = count)) +
  geom_line() + geom_smooth(span = 5, se = F) +
  #geom_point() + geom_smooth(span = 5, se = F) +
  scale_x_continuous(name = 'Year', breaks = seq(1975, 2020, by = 5)) +
  scale_y_continuous(name = 'Count of Working Papers')
```

The series has constantly been growing as a source of presentable research in-progress.
It is unclear whether the gowth in size of this listing is because of the rise in 
release of working papers in general, or because of an individual rise in the use of the NBER series.
It is clear, however, that the internet has been a major part of disseminating research in-progress,
as before the online listing of working papers researchers would only have the chance to read colleague manuscripts in person
or at conferences.  Today we can take a phone from our pockets, google 
something like ``NBER working paper on economics publishing'' and read cutting edge research before it is even published.
I will make a note to count the number of working papers from all sources with the next data source, 
to measure rise in internet listing of working papers in general.

The papers are sorted in to subjects areas, the [NBER research area programmes](https://www.nber.org/programs/) (the graph below only shows top 10 areas).
Note: papers can be cross registered to multiple areas, and are more likely to be so for recent years.

```{r, echo=F, fig.height = 4, fig.width = 7, fig.align = 'center'}
# SHow count per year, for each area
Papers_subjects.data <- Papers.data %>% transmute(
  aging = stri_detect_fixed(nber_area, 'Program on the Economics of Aging'),
  assets = stri_detect_fixed(nber_area, 'Asset Pricing Program'),
  corporate_finance = stri_detect_fixed(nber_area, 'Corporate Finance Program'),
  children = stri_detect_fixed(nber_area, 'Program on Children'),
  american_history = stri_detect_fixed(nber_area, 'Program on the Development of the American Economy'),
  development = stri_detect_fixed(nber_area, 'Development Economics Progra'),
  education = stri_detect_fixed(nber_area, 'Economics of Education Program'),
  energy = stri_detect_fixed(nber_area, 'Environment and Energy Program'),
  growth = stri_detect_fixed(nber_area, 'Economic Fluctuations and Growth Program'),
  care = stri_detect_fixed(nber_area, 'Health Care Program'),
  health = stri_detect_fixed(nber_area, 'Health Economics Program'),
  finance_macro = stri_detect_fixed(nber_area, 'International Finance and Macroeconomics Program'),
  indust_org = stri_detect_fixed(nber_area, 'Industrial Organization Program'),
  trade_invest = stri_detect_fixed(nber_area, 'International Trade and Investment Program'),
  law = stri_detect_fixed(nber_area, 'Law and Economics Program'),
  labour = stri_detect_fixed(nber_area, 'Labor Studies Program'),
  monetary = stri_detect_fixed(nber_area, 'Monetary Economics Program'),
  public = stri_detect_fixed(nber_area, 'Public Economics Program'),
  poli_econ = stri_detect_fixed(nber_area, 'Political Economy Program'),
  productivity = stri_detect_fixed(nber_area, 'Productivity, Innovation, and Entrepreneurship Program'),
  date = as.Date(date),
  year = as.integer(format(date, format = '%Y'))) %>%
  group_by(year) %>%
  summarise(count = n(),
            aging = sum(aging),
            assets = sum(assets),
            corporate_finance = sum(corporate_finance),
            children = sum(children),
            american_history = sum(american_history),
            development = sum(development),
            education = sum(education),
            energy = sum(energy),
            growth = sum(growth),
            care = sum(care),
            health = sum(health),
            finance_macro = sum(finance_macro),
            indust_org = sum(indust_org),
            trade_invest = sum(trade_invest),
            law = sum(law),
            labour = sum(labour),
            monetary = sum(monetary),
            public = sum(public),
            poli_econ = sum(poli_econ),
            productivity = sum(productivity)) %>%
  reshape(timevar = 'nber_area',
          times = c('aging', 'assets', 'corporate_finance', 'children', 'american_history', 'development', 'education',
                      'energy', 'growth', 'care', 'health', 'finance_macro', 'indust_org', 'trade_invest', 'law', 'labour', 
                      'monetary', 'public', 'poli_econ', 'productivity'),
          varying = c('aging', 'assets', 'corporate_finance', 'children', 'american_history', 'development', 'education',
                      'energy', 'growth', 'care', 'health', 'finance_macro', 'indust_org', 'trade_invest', 'law', 'labour', 
                      'monetary', 'public', 'poli_econ', 'productivity'),
          v.names = 'count', direction = 'long', new.row.names = 1:1000) %>% group_by(year)

# Choose most popular areas :
popular_areas <- Papers_subjects.data %>% 
  group_by(nber_area) %>% summarise_all(sum) %>% 
  arrange(-count) %>% head(10) %>% pull(nber_area)

# Graph the count of the the most popular areas
Papers_subjects.data %>%
  filter(nber_area %in% popular_areas) %>%
  ggplot(aes(x = year, y = count, group = nber_area, colour = nber_area)) +
  geom_line() +
  scale_x_continuous(name = 'Year', breaks = seq(1975, 2015, by = 5)) +
  scale_y_continuous(name = 'Count of Working Papers')
```

Consider some big revisions to this graph, perhaps a cdf plot would be better, easier to differentiate areas.

To-do:
Embed with quotes of famous working papers, and set the stage for a difference between
published and un-published papers.


### Publication

The majority of papers in every year go on to be published.
The most recent years (2015 and on) have a very low publishing rate -- papers likely to yet be published?

```{r, echo=F}
# Show rate of publishing by year
Papers.data %>%
  transmute(publish_rate = (published != ''),
            date         = as.Date(date)) %>%
  mutate(year = as.integer(format(date, '%Y'))) %>%
  filter(year < 2015) %>%
  group_by(year) %>%
  summarise(publish_rate = mean(publish_rate)) %>%
  ggplot(aes(x = year, y = publish_rate)) +
  geom_point() +
  geom_smooth(span = 5) +
  theme_bw() +
  scale_x_continuous(name = 'Year', breaks = seq(1975, 2015, by = 5)) +
  scale_y_continuous(name = 'Publish Rate (%)', 
                     breaks = seq(0, 1, by = 0.2), limits = c(0,1))
# Note sharp drops off 2015, work still yet to be published?
```

### To-do: Textual difference

Textual analysis along the publication outcome:

Let's compare textual measures (sentiment, relative word usage, measure of mathematical/empirical words) across dimensions:
published vs unpublished.

Examine the outcome within groups -> are there any NBER programmes we would expect to have changed specifically?  
Perhaps the rise in empiricism is more pronounced in labour than, say, macroeconomics
and that this can be tested <i>empirically</i>.



### Conclusion: the wider economics publishing pipeline

Write about the starting point of my wider research project.
